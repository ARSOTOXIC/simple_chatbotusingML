{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFUmlPdOkeBd",
        "outputId": "49dbc601-69db-4537-9811-e7cc5b6abc1b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V48HJDOZYkti"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import nltk\n",
        " #natural launguage processing\n",
        "import string #process the string \n",
        "import random #to select random values from the list "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5GbXjKQuYcN",
        "outputId": "842d0d74-06f9-48a2-f340-8fe3cbf8395d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('yourdataset.txt', 'r',errors =\"ignore\")  #open the dataset file\n",
        "#read the data set\n",
        "raw_doc=f.read()      \n",
        "#Converts text to lowercase \n",
        "raw_doc=raw_doc.lower() \n",
        "# it divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words,\n",
        "# collocations, and words that start sentences.\n",
        "nltk.download(\"punkt\")  \n",
        "nltk.download('wordnet') #Using the WordNet dictionary \n",
        "sent_tokens=nltk.sent_tokenize(raw_doc) # sentencetokens Converts doc to list of sentences--split document into list of sentence \n",
        "word_tokens=nltk.word_tokenize(raw_doc) #wordtokens Converts doc to list of words -- split document into list of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "2KhCgDSDZZ3B",
        "outputId": "1632df01-f13a-4348-be2f-d8abc6b56a9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c36d6f02e7fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'borabot.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#open the dataset file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#read the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw_doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Converts text to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mraw_doc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'borabot.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens[:1] #sentence token"
      ],
      "metadata": {
        "id": "bIkZkcwMagiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:40] #word tokenization"
      ],
      "metadata": {
        "id": "g5odxYkbc_aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text preprosseing \n"
      ],
      "metadata": {
        "id": "3CMQBOmEd7h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer() #wordnet is a dictionary that composed with nltk lemmetization is basically preprocesssing\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        " return [lemmer.lemmatize(token) for token in tokens] \n",
        "remove_punct_dict = dict((ord (punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens (nltk.word_tokenize (text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "DME-Jm8kdNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS=(\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",\"good morning\") \n",
        "GREET_RESPONSES = [\"hi\", \"hey\", \"nods\", \"hi there\", \"hello\",\"good morning\",\"I am glad! You are talking to me\",\"auu sabu bhala\"] \n",
        " \n",
        "GREET_INPUTS1=(\" auu sabu bhala ta \") \n",
        "GREET_RESPONSES1 = [\"han sabu bhala tame kuha \"] \n",
        "#here the random liberary is used so that it can choose any of the response to the random to the users inputs\n",
        "def greet (sentence):\n",
        "  for word in sentence.split():\n",
        "    if word. lower() in GREET_INPUTS:\n",
        "       return random.choice(GREET_RESPONSES)\n",
        "       if word. lower() in GREET_INPUTS1:\n",
        "         return random.choice(GREET_RESPONSES1)"
      ],
      "metadata": {
        "id": "6Don5NcDefRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer #tfidf - termfrequency(number of time a word is repeated in the sentence or the frequency of occurance of the word)// inverse term frequency(rarierity  of the word in the document ) \n",
        "from sklearn.metrics.pairwise import cosine_similarity  #one we have everything in 1s and 0s it gives us a normalise output \n"
      ],
      "metadata": {
        "id": "W0Oe1YPngxCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "4oIQIg5uknBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response (user_response):\n",
        "  robo1_response =''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity (tfidf[-1], tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat = vals. flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"I am sorry! I don't understand you, can you repeat it \"\n",
        "    return robo1_response\n",
        "  else :\n",
        "    robo1_response = robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "Xr1fgfWhhl6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ywt-JKIXTaHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print(\"BOT: HEY MY NAME IS TBOT\")\n",
        "print(\"Also, if you want to exit any time, just type Bye!\") \n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response=user_response.lower()  \n",
        "  if(user_response!='bye'):\n",
        "      if(user_response==' thanks' or user_response==' thank you' ):\n",
        "       flag = False\n",
        "       print(\"BOT: You are welcome..\")\n",
        "      else:\n",
        "       if(greet(user_response)!=None): \n",
        "         print(\"BOT: \"+greet (user_response))\n",
        "       else:\n",
        "         sent_tokens.append(user_response)\n",
        "         word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
        "         final_words = list(set(word_tokens)) \n",
        "         print(\"BOT: \",end=\"\")\n",
        "         print(response(user_response))\n",
        "         sent_tokens.remove(user_response)\n",
        "  else:\n",
        "      flag = False\n",
        "      print(\"BOT: GoodBye!!! Jai Shree Ram \")"
      ],
      "metadata": {
        "id": "X6y1VIuDmdok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECYJiN_tjfy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}